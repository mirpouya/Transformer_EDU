{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO42PpA2eSg2gQYFmkTkZKH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mirpouya/Transformer_EDU/blob/main/einsum_and_Transformers.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Think about the situation that you want to merge d dimsnsion of a 4D matrix"
      ],
      "metadata": {
        "id": "LSPFzSl9biHm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "QccsvTiNbcSe"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rand_tensor = torch.rand((2, 5))\n",
        "rand_tensor"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m915A_Bob1jk",
        "outputId": "ccf8bc4e-fa59-45f0-b5cf-35a69c6f45dd"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.6676, 0.1100, 0.2785, 0.7718, 0.5302],\n",
              "        [0.0416, 0.1539, 0.6134, 0.4935, 0.7339]])"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rand_tensor = torch.rand((3,4,5,6))\n",
        "rand_tensor"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XzLHL-fIb_5B",
        "outputId": "98587a61-fd73-4570-c594-f0ceaa8f5d67"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[[0.8489, 0.0483, 0.5378, 0.9894, 0.8566, 0.1870],\n",
              "          [0.8803, 0.8082, 0.5261, 0.5652, 0.4972, 0.6773],\n",
              "          [0.7561, 0.7341, 0.8577, 0.0951, 0.7072, 0.9461],\n",
              "          [0.4221, 0.3148, 0.6844, 0.0288, 0.5869, 0.3265],\n",
              "          [0.3065, 0.2786, 0.1006, 0.7117, 0.0231, 0.4508]],\n",
              "\n",
              "         [[0.1693, 0.1754, 0.7354, 0.8866, 0.1924, 0.3853],\n",
              "          [0.8709, 0.8777, 0.3474, 0.8770, 0.4972, 0.2747],\n",
              "          [0.1752, 0.2968, 0.5154, 0.9932, 0.3690, 0.2059],\n",
              "          [0.8455, 0.4737, 0.0228, 0.3239, 0.1615, 0.1288],\n",
              "          [0.4957, 0.5910, 0.8496, 0.5045, 0.4531, 0.3430]],\n",
              "\n",
              "         [[0.3197, 0.1727, 0.3890, 0.9110, 0.7578, 0.2692],\n",
              "          [0.7065, 0.7479, 0.7896, 0.9538, 0.3639, 0.0738],\n",
              "          [0.7084, 0.0436, 0.2143, 0.8620, 0.3914, 0.5374],\n",
              "          [0.3876, 0.7103, 0.1160, 0.5582, 0.3218, 0.0496],\n",
              "          [0.9270, 0.0134, 0.5143, 0.6567, 0.3753, 0.1027]],\n",
              "\n",
              "         [[0.3410, 0.0578, 0.9611, 0.3014, 0.6937, 0.9209],\n",
              "          [0.0501, 0.8220, 0.8667, 0.7563, 0.3599, 0.6416],\n",
              "          [0.5499, 0.3292, 0.6693, 0.0279, 0.8807, 0.2488],\n",
              "          [0.7403, 0.4457, 0.1138, 0.7644, 0.3542, 0.0286],\n",
              "          [0.6503, 0.6968, 0.8842, 0.6054, 0.5093, 0.8081]]],\n",
              "\n",
              "\n",
              "        [[[0.3174, 0.3138, 0.3092, 0.0158, 0.0256, 0.9286],\n",
              "          [0.8884, 0.1786, 0.3870, 0.8737, 0.8451, 0.2290],\n",
              "          [0.9142, 0.1690, 0.5613, 0.0051, 0.2109, 0.6393],\n",
              "          [0.3755, 0.2340, 0.5363, 0.4666, 0.9711, 0.7027],\n",
              "          [0.9163, 0.8125, 0.2639, 0.9211, 0.9749, 0.9738]],\n",
              "\n",
              "         [[0.5985, 0.7766, 0.1102, 0.1456, 0.3525, 0.5763],\n",
              "          [0.5913, 0.8110, 0.2274, 0.3796, 0.0270, 0.5763],\n",
              "          [0.0080, 0.6456, 0.8433, 0.8859, 0.0211, 0.7383],\n",
              "          [0.5375, 0.2426, 0.2502, 0.2597, 0.4523, 0.7688],\n",
              "          [0.6661, 0.5332, 0.1410, 0.7861, 0.9011, 0.9067]],\n",
              "\n",
              "         [[0.8185, 0.4468, 0.4491, 0.9281, 0.8075, 0.1576],\n",
              "          [0.6541, 0.6574, 0.5085, 0.8808, 0.8834, 0.1631],\n",
              "          [0.3311, 0.6191, 0.5944, 0.5690, 0.1974, 0.7862],\n",
              "          [0.7847, 0.8504, 0.6643, 0.4120, 0.9691, 0.7709],\n",
              "          [0.0453, 0.4083, 0.3387, 0.3327, 0.0505, 0.2115]],\n",
              "\n",
              "         [[0.1383, 0.9834, 0.4837, 0.2883, 0.2132, 0.2426],\n",
              "          [0.5087, 0.6237, 0.3395, 0.6714, 0.6215, 0.6109],\n",
              "          [0.0753, 0.1145, 0.4924, 0.1240, 0.8465, 0.8960],\n",
              "          [0.2254, 0.3746, 0.1446, 0.9280, 0.3768, 0.2352],\n",
              "          [0.9897, 0.0367, 0.5396, 0.0034, 0.3628, 0.3668]]],\n",
              "\n",
              "\n",
              "        [[[0.9920, 0.4072, 0.6612, 0.5761, 0.4168, 0.7882],\n",
              "          [0.2216, 0.2300, 0.1920, 0.7678, 0.6911, 0.6808],\n",
              "          [0.4595, 0.3091, 0.5630, 0.9103, 0.4920, 0.7450],\n",
              "          [0.4372, 0.9884, 0.3922, 0.2061, 0.5275, 0.9352],\n",
              "          [0.6170, 0.6797, 0.5567, 0.6605, 0.8710, 0.6113]],\n",
              "\n",
              "         [[0.4416, 0.3784, 0.5769, 0.5161, 0.9388, 0.6164],\n",
              "          [0.6011, 0.7685, 0.8005, 0.1465, 0.6471, 0.8613],\n",
              "          [0.4207, 0.5140, 0.6857, 0.3888, 0.4837, 0.6243],\n",
              "          [0.3598, 0.0748, 0.4748, 0.2493, 0.8844, 0.1068],\n",
              "          [0.2721, 0.7764, 0.1199, 0.8618, 0.8246, 0.5745]],\n",
              "\n",
              "         [[0.2369, 0.8772, 0.0551, 0.2446, 0.8624, 0.6197],\n",
              "          [0.7136, 0.7686, 0.2383, 0.2115, 0.8459, 0.2057],\n",
              "          [0.5024, 0.3434, 0.6752, 0.1034, 0.5450, 0.5130],\n",
              "          [0.3947, 0.0909, 0.9298, 0.0661, 0.9765, 0.8199],\n",
              "          [0.4717, 0.7322, 0.8239, 0.4710, 0.0139, 0.9589]],\n",
              "\n",
              "         [[0.4998, 0.3634, 0.4727, 0.6080, 0.8312, 0.0060],\n",
              "          [0.3112, 0.6864, 0.4245, 0.7543, 0.4704, 0.8260],\n",
              "          [0.0806, 0.4791, 0.0324, 0.2553, 0.7148, 0.3027],\n",
              "          [0.9425, 0.8949, 0.9603, 0.2458, 0.1247, 0.1649],\n",
              "          [0.5484, 0.8661, 0.4687, 0.0661, 0.5691, 0.5003]]]])"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install einops"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bHUGQgJDurLs",
        "outputId": "3e6c114e-defa-4ca2-8214-93f3e2b9c346"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: einops in /usr/local/lib/python3.10/dist-packages (0.6.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from einops import rearrange, reduce, repeat"
      ],
      "metadata": {
        "id": "lY_hSuTEth8L"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rand_tensor.size()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DtZy-gqnvNMr",
        "outputId": "c1176a09-d297-402f-e55a-a0318ce8592f"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([3, 4, 5, 6])"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rand_tensor = rearrange(rand_tensor, \"b c h w -> (b w) c h\")\n",
        "\n",
        "# b = 3, c = 4, h = 5, w = 6\n",
        "# to (3 * 6, 4, 5)"
      ],
      "metadata": {
        "id": "dTic0P1fu1ys"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rand_tensor.size()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LwpB-2HwvIRs",
        "outputId": "f7c5dfa6-992a-42bd-be54-7d6603cfd7ea"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([18, 4, 5])"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### <b> einsum </b>"
      ],
      "metadata": {
        "id": "JE8HYgDKvzLu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "a = torch.rand((10, 10, 30))\n",
        "a.size()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RKuK0vagvKPd",
        "outputId": "14e95831-d4de-46d4-9737-563b23739989"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([10, 10, 30])"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "a = torch.rand(10, 20, 30)\n",
        "a.size()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f92IsGV5xz-9",
        "outputId": "71fe8431-2e76-4649-974f-a2d578ba7ed2"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([10, 20, 30])"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "a = torch.randn(10, 20, 30)  # b = 10, i = 20, k = 30\n",
        "b = torch.randn(10, 50, 30)  # b = 10, i = 50, k = 30"
      ],
      "metadata": {
        "id": "pc3i2qQAx8Sy"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y = torch.einsum(\"b i k, b j k -> b i j\", a, b)"
      ],
      "metadata": {
        "id": "byKYLAJwyDXp"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y.size()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BjyYVOn8zw3S",
        "outputId": "991f6f2b-5c58-4523-83bf-b0606de07c2d"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([10, 20, 50])"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "A = np.array([0, 1, 2])\n",
        "\n",
        "B = np.array([[ 0,  1,  2,  3],\n",
        "              [ 4,  5,  6,  7],\n",
        "              [ 8,  9, 10, 11]])"
      ],
      "metadata": {
        "id": "syj4A0QRzO90"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.einsum('i,ij->i', A, B)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ILoYfE8-zVhN",
        "outputId": "afbb4b2a-fff3-45d2-97c2-94b29eb9b6da"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 0, 22, 76])"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# <b> Transformer's blocks implementation </b>"
      ],
      "metadata": {
        "id": "KoB3r7180w9U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <b> Scaled dot product self-attention </b>"
      ],
      "metadata": {
        "id": "t14xZSEi1QnJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the article, after embedding and positional embedding, embeddings are multiplied by weight matrices  <b> W_Q </b>,  <b> W_K </b>,  <b> W_V </b>."
      ],
      "metadata": {
        "id": "LsEIl3-U1Zvt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "the input <b> X </b> to the attention block is of shape: (batch, sequence_len, embedding_dim)"
      ],
      "metadata": {
        "id": "mLLcZraD2Sov"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The matrix multiplication happens in the <b> embedding_dim </b> dimension, regardless of batch_size and sequence_len"
      ],
      "metadata": {
        "id": "KJh9ObhI2j1Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# linear projection before attention block\n",
        "embed_dim = 512\n",
        "input = torch.randn(10, 12, 512)   # for example\n",
        "\n",
        "qkv_weights = nn.Linear(embed_dim, embed_dim * 3, bias = False)  # we concat q, k, v into one matrix, then multiply by qkv_weights\n",
        "\n",
        "qkv = qkv_weights(input)\n",
        "\n",
        "q, k, v = tuple(rearrange(qkv, \"b t (d k) -> k b t d\", k = 3))"
      ],
      "metadata": {
        "id": "V2XlrIKQzYXS"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "q.size(), k.size(), v.size()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5kogmTq039Ac",
        "outputId": "4d76ebab-ad3f-4d8e-fc2a-8f4420148a17"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([10, 12, 512]),\n",
              " torch.Size([10, 12, 512]),\n",
              " torch.Size([10, 12, 512]))"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<b> Step 2 </b>"
      ],
      "metadata": {
        "id": "ouNjbWSJ6V2S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "calculate scaled dot product, apply mask, and finally compute softmax in d last dimension."
      ],
      "metadata": {
        "id": "32f77Wz36ZnR"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XUBdVuGu4AO2"
      },
      "execution_count": 17,
      "outputs": []
    }
  ]
}