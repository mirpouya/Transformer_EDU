{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNbEDZTI7yC7JSyQQGhMD9J",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mirpouya/Transformer_EDU/blob/main/einsum_and_Transformers.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Think about the situation that you want to merge d dimsnsion of a 4D matrix"
      ],
      "metadata": {
        "id": "LSPFzSl9biHm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "QccsvTiNbcSe"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rand_tensor = torch.rand((2, 5))\n",
        "rand_tensor"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m915A_Bob1jk",
        "outputId": "f39ebb61-b9f5-42bd-9f80-456df29608b5"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.9049, 0.7497, 0.7549, 0.6756, 0.6255],\n",
              "        [0.5269, 0.8023, 0.4166, 0.6086, 0.3092]])"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rand_tensor = torch.rand((3,4,5,6))\n",
        "rand_tensor"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XzLHL-fIb_5B",
        "outputId": "34acb9f0-8b68-4f47-b50b-2da0c23e15b4"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[[0.4272, 0.5442, 0.2517, 0.7889, 0.8619, 0.8654],\n",
              "          [0.0666, 0.8560, 0.9473, 0.5588, 0.7690, 0.0586],\n",
              "          [0.2043, 0.5678, 0.0451, 0.5862, 0.7480, 0.0267],\n",
              "          [0.4652, 0.9349, 0.4918, 0.2418, 0.1082, 0.6770],\n",
              "          [0.5552, 0.8100, 0.4213, 0.9764, 0.0273, 0.5012]],\n",
              "\n",
              "         [[0.3132, 0.1171, 0.7283, 0.0282, 0.3125, 0.9177],\n",
              "          [0.4310, 0.5009, 0.7681, 0.8635, 0.2355, 0.2121],\n",
              "          [0.4530, 0.6935, 0.4342, 0.6764, 0.0459, 0.6705],\n",
              "          [0.2524, 0.1069, 0.0038, 0.3601, 0.7893, 0.1103],\n",
              "          [0.5747, 0.5341, 0.8096, 0.3766, 0.9147, 0.8954]],\n",
              "\n",
              "         [[0.4420, 0.8605, 0.9480, 0.3037, 0.4664, 0.0353],\n",
              "          [0.5814, 0.1024, 0.8893, 0.5554, 0.9763, 0.5051],\n",
              "          [0.2044, 0.1190, 0.9412, 0.3974, 0.9323, 0.9973],\n",
              "          [0.8388, 0.6731, 0.2284, 0.9694, 0.1819, 0.4629],\n",
              "          [0.3343, 0.2152, 0.6220, 0.7947, 0.6908, 0.8189]],\n",
              "\n",
              "         [[0.8629, 0.9964, 0.0746, 0.0200, 0.2734, 0.8139],\n",
              "          [0.2242, 0.7166, 0.0622, 0.8143, 0.6598, 0.6455],\n",
              "          [0.3716, 0.3907, 0.3656, 0.2091, 0.7882, 0.6235],\n",
              "          [0.6762, 0.5964, 0.9145, 0.5889, 0.8396, 0.8899],\n",
              "          [0.5991, 0.9620, 0.4295, 0.9265, 0.3233, 0.9774]]],\n",
              "\n",
              "\n",
              "        [[[0.9018, 0.2438, 0.9898, 0.7857, 0.8679, 0.0168],\n",
              "          [0.3096, 0.1361, 0.9589, 0.4670, 0.2217, 0.9792],\n",
              "          [0.7132, 0.4019, 0.3470, 0.9086, 0.4235, 0.6478],\n",
              "          [0.4669, 0.5753, 0.6009, 0.7378, 0.7496, 0.6913],\n",
              "          [0.1694, 0.2090, 0.8577, 0.9655, 0.3348, 0.1392]],\n",
              "\n",
              "         [[0.9513, 0.2524, 0.7922, 0.3270, 0.0387, 0.3182],\n",
              "          [0.2469, 0.4797, 0.6442, 0.4510, 0.1009, 0.2812],\n",
              "          [0.6588, 0.2160, 0.2359, 0.8409, 0.5333, 0.2596],\n",
              "          [0.4071, 0.0415, 0.7538, 0.3667, 0.8669, 0.8915],\n",
              "          [0.8667, 0.0047, 0.7574, 0.5372, 0.6091, 0.6900]],\n",
              "\n",
              "         [[0.6398, 0.5792, 0.1008, 0.3688, 0.2049, 0.6364],\n",
              "          [0.8297, 0.1933, 0.5258, 0.8563, 0.8649, 0.7843],\n",
              "          [0.2657, 0.2444, 0.9307, 0.8575, 0.2388, 0.2636],\n",
              "          [0.0790, 0.1347, 0.2667, 0.8244, 0.4092, 0.8911],\n",
              "          [0.6644, 0.6058, 0.5153, 0.8025, 0.2875, 0.5514]],\n",
              "\n",
              "         [[0.8416, 0.1868, 0.2209, 0.1332, 0.9909, 0.5976],\n",
              "          [0.1745, 0.9798, 0.7110, 0.5886, 0.4999, 0.8232],\n",
              "          [0.8318, 0.5812, 0.3809, 0.9237, 0.6479, 0.4829],\n",
              "          [0.7117, 0.3204, 0.2867, 0.5706, 0.1116, 0.6337],\n",
              "          [0.1863, 0.2549, 0.3659, 0.6566, 0.4473, 0.7069]]],\n",
              "\n",
              "\n",
              "        [[[0.7864, 0.7505, 0.7933, 0.8403, 0.3045, 0.3888],\n",
              "          [0.6796, 0.7260, 0.5537, 0.4189, 0.2764, 0.6221],\n",
              "          [0.7249, 0.5458, 0.8031, 0.9228, 0.4622, 0.6167],\n",
              "          [0.5789, 0.5723, 0.0372, 0.1574, 0.3982, 0.1119],\n",
              "          [0.8480, 0.7446, 0.4025, 0.5443, 0.8778, 0.2780]],\n",
              "\n",
              "         [[0.1672, 0.0345, 0.7300, 0.3962, 0.3273, 0.9521],\n",
              "          [0.6510, 0.4788, 0.5204, 0.8159, 0.3375, 0.0057],\n",
              "          [0.1566, 0.4103, 0.2925, 0.2726, 0.2700, 0.8671],\n",
              "          [0.7288, 0.1491, 0.6925, 0.0892, 0.8175, 0.8230],\n",
              "          [0.5386, 0.7249, 0.4363, 0.7636, 0.4802, 0.8073]],\n",
              "\n",
              "         [[0.6148, 0.7099, 0.5004, 0.2859, 0.5182, 0.2388],\n",
              "          [0.5599, 0.4538, 0.5293, 0.7593, 0.3754, 0.6179],\n",
              "          [0.6391, 0.8924, 0.4367, 0.8894, 0.0152, 0.5887],\n",
              "          [0.2808, 0.0929, 0.4497, 0.1288, 0.7308, 0.1747],\n",
              "          [0.6893, 0.0951, 0.3678, 0.0202, 0.5985, 0.5977]],\n",
              "\n",
              "         [[0.8617, 0.0382, 0.3204, 0.5672, 0.8926, 0.2654],\n",
              "          [0.3726, 0.2008, 0.0641, 0.5963, 0.2707, 0.9935],\n",
              "          [0.8038, 0.8759, 0.6929, 0.4803, 0.0618, 0.3731],\n",
              "          [0.4023, 0.1179, 0.9738, 0.1390, 0.3190, 0.1608],\n",
              "          [0.0787, 0.7271, 0.8138, 0.3474, 0.8727, 0.1744]]]])"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install einops"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bHUGQgJDurLs",
        "outputId": "337263b0-cf57-4bc8-ff90-0de0afaa702f"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: einops in /usr/local/lib/python3.10/dist-packages (0.6.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from einops import rearrange, reduce, repeat"
      ],
      "metadata": {
        "id": "lY_hSuTEth8L"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rand_tensor.size()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DtZy-gqnvNMr",
        "outputId": "741cb723-2f9f-45b8-87cf-e1d6b5d5fa4d"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([3, 4, 5, 6])"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rand_tensor = rearrange(rand_tensor, \"b c h w -> (b w) c h\")\n",
        "\n",
        "# b = 3, c = 4, h = 5, w = 6\n",
        "# to (3 * 6, 4, 5)"
      ],
      "metadata": {
        "id": "dTic0P1fu1ys"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rand_tensor.size()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LwpB-2HwvIRs",
        "outputId": "5e5ea2f6-d55b-4343-8592-26c3ebfb3c20"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([18, 4, 5])"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### <b> einsum </b>"
      ],
      "metadata": {
        "id": "JE8HYgDKvzLu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "a = torch.rand((10, 10, 30))\n",
        "a.size()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RKuK0vagvKPd",
        "outputId": "78754137-70f2-458b-b04e-acf1bc464008"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([10, 10, 30])"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "a = torch.rand(10, 20, 30)\n",
        "a.size()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f92IsGV5xz-9",
        "outputId": "eeab173f-9641-42c3-a36f-315eefb76b39"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([10, 20, 30])"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "a = torch.randn(10, 20, 30)  # b = 10, i = 20, k = 30\n",
        "b = torch.randn(10, 50, 30)  # b = 10, i = 50, k = 30"
      ],
      "metadata": {
        "id": "pc3i2qQAx8Sy"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y = torch.einsum(\"b i k, b j k -> b i j\", a, b)"
      ],
      "metadata": {
        "id": "byKYLAJwyDXp"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y.size()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BjyYVOn8zw3S",
        "outputId": "3c554996-cf41-4803-fa35-59d71e08d58c"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([10, 20, 50])"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "A = np.array([0, 1, 2])\n",
        "\n",
        "B = np.array([[ 0,  1,  2,  3],\n",
        "              [ 4,  5,  6,  7],\n",
        "              [ 8,  9, 10, 11]])"
      ],
      "metadata": {
        "id": "syj4A0QRzO90"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.einsum('i,ij->i', A, B)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ILoYfE8-zVhN",
        "outputId": "adf5e024-b76e-4044-85bb-b9a7ccea4af0"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 0, 22, 76])"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# <b> Transformer's blocks implementation </b>"
      ],
      "metadata": {
        "id": "KoB3r7180w9U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <b> Scaled dot product self-attention </b>"
      ],
      "metadata": {
        "id": "t14xZSEi1QnJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the article, after embedding and positional embedding, embeddings are multiplied by weight matrices  <b> W_Q </b>,  <b> W_K </b>,  <b> W_V </b>."
      ],
      "metadata": {
        "id": "LsEIl3-U1Zvt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "the input <b> X </b> to the attention block is of shape: (batch, sequence_len, embedding_dim)"
      ],
      "metadata": {
        "id": "mLLcZraD2Sov"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The matrix multiplication happens in the <b> embedding_dim </b> dimension, regardless of batch_size and sequence_len"
      ],
      "metadata": {
        "id": "KJh9ObhI2j1Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# linear projection before attention block\n",
        "embed_dim = 512\n",
        "input = torch.randn(10, 12, 512)   # for example\n",
        "\n",
        "qkv_weights = nn.Linear(embed_dim, embed_dim * 3, bias = False)  # we concat q, k, v into one matrix, then multiply by qkv_weights\n",
        "\n",
        "qkv = qkv_weights(input)\n",
        "\n",
        "q, k, v = tuple(rearrange(qkv, \"b t (d k) -> k b t d\", k = 3))"
      ],
      "metadata": {
        "id": "V2XlrIKQzYXS"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "q.size(), k.size(), v.size()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5kogmTq039Ac",
        "outputId": "caf0a008-0f07-496a-821b-374b3b5295c1"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([10, 12, 512]),\n",
              " torch.Size([10, 12, 512]),\n",
              " torch.Size([10, 12, 512]))"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<b> Step 2 </b>"
      ],
      "metadata": {
        "id": "ouNjbWSJ6V2S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "calculate scaled dot product, apply mask, and finally compute softmax in d last dimension."
      ],
      "metadata": {
        "id": "32f77Wz36ZnR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# scaled dot product\n",
        "scale_factor = 512 / 8   # embed_dim / num_heads\n",
        "scaled_dot_product = torch.einsum(\"b i d, b j d -> b i j\",q, k) * scale_factor\n",
        "# resulting shape: (batch_size, embed_dim, embed_dim)\n",
        "\n",
        "# masking if needed (decoder)\n",
        "mask = None\n",
        "if mask is not None:\n",
        "  assert mask.shape == scaled_dot_product.shape[1:]\n",
        "  scaled_dot_product = scaled_dot_product.masked_fill(mask, -np.inf)\n",
        "\n",
        "attention = torch.softmax(scaled_dot_product, dim = -1)\n",
        "\n",
        "# multiply attention scores with V\n",
        "print(f\"attention size: {attention.size()}\")\n",
        "print(f\"value size: {v.size()}\")\n",
        "\n",
        "attention_final = torch.einsum(\"b i d, b d j -> b i j\", attention, v)"
      ],
      "metadata": {
        "id": "XUBdVuGu4AO2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3afbbc3b-5a10-40dd-d0fd-570239469519"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "attention size: torch.Size([10, 12, 12])\n",
            "value size: torch.Size([10, 12, 512])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <b> Implementation of Scaled dot Prodcut Self-Attention </b>"
      ],
      "metadata": {
        "id": "PMWG8hwjgFq_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "import numpy\n",
        "from einops import rearrange"
      ],
      "metadata": {
        "id": "DMeN12Xkfe2Z"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SelfAttention(nn.Module):\n",
        "\n",
        "  def __init__(self, embed_dim):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        embed_dim: embedding dimension, with 512 as default\n",
        "        the last dimension size that is provided in forward(x), where x is a 3D tensor\n",
        "    \"\"\"\n",
        "    super().__init__()\n",
        "\n",
        "    # wieght matrices for query, key, and value\n",
        "    self.qkv_weights = nn.Linear(embed_dim, embed_dim * 3, bias = False)\n",
        "    self.scale_factor = embed_dim ** (-0.5)\n",
        "\n",
        "  # forward method\n",
        "  def forward(self, x, mask = None):\n",
        "    assert x.dim() == 3  # x must be a 3D tensor (batch_size, sequence_len, embed_dim)\n",
        "\n",
        "    # step 1\n",
        "    qkv = self.qkv_weights(x)\n",
        "\n",
        "    # step 2: decomposing to q, k, v\n",
        "    # rearranging to [3, batch_size, sequence_len, embed_dim]\n",
        "    q, k , v = tuple(rearrange(qkv, \"b t (d k) -> k b t d\", k = 3))\n",
        "\n",
        "    # scaled_dot_product\n",
        "    scaled_dot_product = torch.einsum(\"b i d, b j d -> b i j\", q, k) * self.scale_factor\n",
        "\n",
        "    # mask attention\n",
        "    if mask is not None:\n",
        "      assert mask.shape == x.shape[1:]\n",
        "      scaled_dot_product = scaled_dot_product.masked_fill(mask, -np.inf)\n",
        "\n",
        "    attention = torch.softmax(scaled_dot_product, dim = -1)\n",
        "    # you have to multiply V in the dimension you apply the softmax. Be careful of that.\n",
        "    attention_final = torch.einsum(\"b i d, b d k -> b i k\")\n",
        "\n",
        "    return attention_final"
      ],
      "metadata": {
        "id": "DNridq4ihU9D"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <b> Implementing Multi-Head_Self-Attention </b>"
      ],
      "metadata": {
        "id": "-BC8vz4Jqq6o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "in a single head case, we project the embedded and positioonal encoded input to weight matrix of size (embed_dim, embed_dim * 3)"
      ],
      "metadata": {
        "id": "g0CWPMRqq6Rm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "in multi-head case, we project the inuput matrix to weigh matrix of size (embed_dim, head_dim * n_heads * 3), which is the same, but in the `rearrange()` step it is easy to separate heads."
      ],
      "metadata": {
        "id": "QXCxh4fOrNTj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "head_dim = 512 // 8\n",
        "n_heads = 8\n",
        "x = torch.randn(10, 12, 512)\n",
        "\n",
        "qkv_weights = nn.Linear(embed_dim, head_dim * n_heads * 3, bias=False)\n",
        "qkv = qkv_weights(x)"
      ],
      "metadata": {
        "id": "6AM9ST-TnKgs"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# decompose qkv to q, k, v\n",
        "q, k, v = tuple(rearrange(qkv, \"b s (h n k) -> k b n s h\", k = 3, n = n_heads))"
      ],
      "metadata": {
        "id": "rkIt0sDPtTrG"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "qkv.size()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l44RQaqkuRGN",
        "outputId": "4dab006a-c0fd-417b-e8b8-69c1a2c960ed"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([10, 12, 1536])"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "q.size(), k.size(), v.size()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_unn1LKeuSiX",
        "outputId": "cf80889b-ef0e-419e-89e6-3ee99040102b"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([10, 8, 12, 64]),\n",
              " torch.Size([10, 8, 12, 64]),\n",
              " torch.Size([10, 8, 12, 64]))"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "the next step is to calculate `scaled-dot-product`, apply the mask, and finally compute the `softmax` in `dim_head`"
      ],
      "metadata": {
        "id": "1vbqDUcIyLnd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# matrix multiplication of q and v in heads\n",
        "# q, k -> (batch_size, n_heads, seq_len, dim_head)\n",
        "scaled_dot_product = torch.einsum(\"b n s d, b n t d -> b n s t\", q, k) * scale_factor   # (batch_size, n_heads, seq_len, tokens) \"seq_len = tokens\"\n",
        "\n",
        "if mask is not None:\n",
        "  # check mask shape\n",
        "  assert mask.shape == scaled_dot_product.shape[2:]\n",
        "  scaled_dot_product = scaled_dot_product.masked_fill(mask, -np.inf)\n",
        "\n",
        "attention = torch.softmax(scaled_dot_product, dim = -1)"
      ],
      "metadata": {
        "id": "bG6HfP-wu8wM"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "now that the attention is computed, we must multiply the attention score of each head with the corresponding value of each head"
      ],
      "metadata": {
        "id": "aUssAlQh0M3U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# attention shape : (batch_size, n_heads, sentence_words, sentence_words)\n",
        "# value shape : (batch_size, n_heads, sentence_words, head_dim)\n",
        "\n",
        "out = torch.einsum(\"b n i j, b n j d -> b n i d\", attention, v)"
      ],
      "metadata": {
        "id": "dbyUz-150KI8"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "it's time to merge heads into one matrix and multiply it by W_O"
      ],
      "metadata": {
        "id": "xvDW-T_v2WUi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# concatenating heads\n",
        "out = rearrange(out, \"b n i d -> b i (n d)\")"
      ],
      "metadata": {
        "id": "Ubsr5GQq2TOi"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "MultiHead(Q, K, V) = Concat(head1, ..., head8) <br>\n",
        "head_i = Attention(QW_Q,i, KW_K,i, VW_V,i)"
      ],
      "metadata": {
        "id": "uFNr4IzA3-Ih"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# apply linear transformation W_O\n",
        "W_O = nn.Linear(embed_dim, embed_dim, bias = False)\n",
        "\n",
        "final_output = W_O(out)"
      ],
      "metadata": {
        "id": "NP-y8RFV3uyd"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <b> Implementation of Multi-Head-Attention </b>"
      ],
      "metadata": {
        "id": "7oUbH_DY5MJ2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from einops import rearrange\n",
        "import numpy as np\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "  def __init__(self, embed_dim, n_heads):\n",
        "    super().__init__()\n",
        "\n",
        "    self.n_heads = n_heads\n",
        "    self.dim_head = embed_dim // self.n_heads\n",
        "    self.embed_dim = self.dim_head * n_heads\n",
        "    self.scale_factor = self.dim_head ** (-0.5)\n",
        "\n",
        "    # weight matrices\n",
        "    self.qkv_weights = nn.Linear(self.embed_dim, n_heads * self.dim_head * 3, bias = False)\n",
        "    self.W_O = nn.Linear(self.embed_dim, self.embed_dim, bias = False)\n",
        "\n",
        "  # forward method\n",
        "  def forward(self, x, mask = None):\n",
        "    # check x has all 3 dimensions -> (batch_size, sentence_length, embedding_dim)\n",
        "    assert x.shape == 3\n",
        "\n",
        "    # step 1: compute query, key, value\n",
        "    qkv = self.qdv_weights(x)   # (batch_size, sentence_length, dim_head * n_heads * 3)\n",
        "\n",
        "    # step 2: decompose to q, k, v\n",
        "    # resulting shape befor tuple():\n",
        "    # [3, n_heads, batch_size, sentence_len, head_dim]\n",
        "    q, k, v = tuple(rearrange(qkv, \"b s (d n k) -> k b n s d\", k = 3, h = self.n_heads))\n",
        "\n",
        "    # step 3: compute scaled_dot_product\n",
        "    scaled_dot_product = torch.einsum(\"b n s d, h n t d -> b n s t\", q, v) * self.scale_factor\n",
        "\n",
        "    # mask if needed\n",
        "    if mask is not None:\n",
        "      assert mask.shape == scaled_dot_product[2:]\n",
        "      scaled_dot_product = scaled_dot_product.masked_fill(mask, -np.inf)\n",
        "\n",
        "    attention = torch.softmax(scaled_dot_product, dim = -1)\n",
        "\n",
        "    # step 4: calculate output\n",
        "    out = torch.einsum(\"b n s j, b n j d -> b n s d\", attention, v)\n",
        "\n",
        "    # step 5: merge heads\n",
        "    out = rearrange(out, \"b n i d -> b s (n d)\")\n",
        "\n",
        "    # step 6: apply W_O\n",
        "    output = self.W_O(out)\n",
        "\n",
        "    return output"
      ],
      "metadata": {
        "id": "ez4ngwIJ5KBo"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <b> Transformer Encoder Block </b>"
      ],
      "metadata": {
        "id": "N_u9fXbtOpjF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerBlock(nn.Module):\n",
        "  def __init__(self, embed_dim, n_heads = 8, dim_head = None, dim_linear_block = 1024, droupout_rate = 0.1):\n",
        "    super().__init__()\n",
        "\n",
        "    self.multi_head_att = MultiHeadAttention(embed_dim=)"
      ],
      "metadata": {
        "id": "-wRGJ3uOL0Bq"
      },
      "execution_count": 29,
      "outputs": []
    }
  ]
}