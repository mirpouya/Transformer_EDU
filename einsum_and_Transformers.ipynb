{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPSuSlJyDrKIDX1tMsSD0kJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mirpouya/Transformer_EDU/blob/main/einsum_and_Transformers.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Think about the situation that you want to merge d dimsnsion of a 4D matrix"
      ],
      "metadata": {
        "id": "LSPFzSl9biHm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "QccsvTiNbcSe"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rand_tensor = torch.rand((2, 5))\n",
        "rand_tensor"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m915A_Bob1jk",
        "outputId": "20dbc53d-e6e9-4b9e-f280-86b5c2c96dd1"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.8782, 0.9865, 0.0878, 0.2703, 0.7359],\n",
              "        [0.1179, 0.7844, 0.8179, 0.5407, 0.9978]])"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rand_tensor = torch.rand((3,4,5,6))\n",
        "rand_tensor"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XzLHL-fIb_5B",
        "outputId": "7f4e909a-0629-4f42-cf06-7393e32feaa9"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[[0.1378, 0.0724, 0.4583, 0.5448, 0.3950, 0.5617],\n",
              "          [0.9874, 0.9202, 0.7649, 0.8667, 0.7688, 0.9397],\n",
              "          [0.7262, 0.6742, 0.6526, 0.2905, 0.5091, 0.9446],\n",
              "          [0.4610, 0.8897, 0.4249, 0.2802, 0.1738, 0.7169],\n",
              "          [0.4922, 0.9954, 0.8705, 0.2483, 0.8635, 0.3960]],\n",
              "\n",
              "         [[0.4481, 0.8084, 0.0134, 0.0274, 0.0634, 0.8233],\n",
              "          [0.7449, 0.4815, 0.3212, 0.9034, 0.5738, 0.1587],\n",
              "          [0.4295, 0.9350, 0.0716, 0.9254, 0.6426, 0.9469],\n",
              "          [0.1661, 0.7564, 0.0913, 0.7325, 0.6468, 0.0161],\n",
              "          [0.1581, 0.6332, 0.1155, 0.3457, 0.6860, 0.8658]],\n",
              "\n",
              "         [[0.5001, 0.7243, 0.4377, 0.8629, 0.0585, 0.5619],\n",
              "          [0.7507, 0.5979, 0.7335, 0.3665, 0.3047, 0.0133],\n",
              "          [0.3859, 0.2861, 0.9227, 0.9089, 0.9121, 0.8607],\n",
              "          [0.6747, 0.2502, 0.4391, 0.7914, 0.9292, 0.6879],\n",
              "          [0.5172, 0.8294, 0.1045, 0.1152, 0.6105, 0.2560]],\n",
              "\n",
              "         [[0.7807, 0.1589, 0.7507, 0.4653, 0.9658, 0.2052],\n",
              "          [0.2367, 0.7955, 0.4479, 0.2829, 0.7847, 0.2800],\n",
              "          [0.1502, 0.7120, 0.4416, 0.6721, 0.5760, 0.4934],\n",
              "          [0.8511, 0.4649, 0.0918, 0.4533, 0.8335, 0.8190],\n",
              "          [0.4094, 0.6532, 0.9834, 0.2937, 0.9507, 0.6677]]],\n",
              "\n",
              "\n",
              "        [[[0.5402, 0.5488, 0.2182, 0.5063, 0.5460, 0.7252],\n",
              "          [0.5533, 0.1881, 0.5163, 0.8555, 0.6638, 0.7224],\n",
              "          [0.5926, 0.7102, 0.1150, 0.7482, 0.8405, 0.5767],\n",
              "          [0.9320, 0.6194, 0.9461, 0.3019, 0.9792, 0.7395],\n",
              "          [0.9385, 0.9057, 0.6882, 0.1416, 0.9468, 0.3388]],\n",
              "\n",
              "         [[0.4149, 0.9567, 0.3817, 0.9752, 0.6175, 0.3414],\n",
              "          [0.0909, 0.2194, 0.7477, 0.8234, 0.1658, 0.6750],\n",
              "          [0.5361, 0.4121, 0.6724, 0.5640, 0.3837, 0.5076],\n",
              "          [0.6790, 0.7122, 0.4003, 0.4908, 0.0138, 0.4481],\n",
              "          [0.4025, 0.8934, 0.6560, 0.3085, 0.5198, 0.1216]],\n",
              "\n",
              "         [[0.6565, 0.1294, 0.5771, 0.4686, 0.3754, 0.9278],\n",
              "          [0.2152, 0.7303, 0.6473, 0.6313, 0.2057, 0.0217],\n",
              "          [0.9090, 0.1708, 0.5749, 0.2170, 0.2693, 0.6991],\n",
              "          [0.0862, 0.5778, 0.4428, 0.8604, 0.9926, 0.9791],\n",
              "          [0.3953, 0.0955, 0.1581, 0.5323, 0.0803, 0.0491]],\n",
              "\n",
              "         [[0.1383, 0.6751, 0.3419, 0.8076, 0.6547, 0.5537],\n",
              "          [0.3530, 0.2712, 0.9914, 0.2712, 0.7147, 0.1304],\n",
              "          [0.2164, 0.5346, 0.2406, 0.2610, 0.1373, 0.4156],\n",
              "          [0.9155, 0.7970, 0.1060, 0.3975, 0.8778, 0.1200],\n",
              "          [0.2176, 0.9131, 0.5545, 0.9436, 0.6123, 0.0417]]],\n",
              "\n",
              "\n",
              "        [[[0.7105, 0.8143, 0.1891, 0.3447, 0.1557, 0.2274],\n",
              "          [0.3599, 0.0769, 0.9997, 0.7799, 0.7433, 0.2985],\n",
              "          [0.7122, 0.9233, 0.2744, 0.7694, 0.6302, 0.6130],\n",
              "          [0.4984, 0.0232, 0.0817, 0.4565, 0.5320, 0.0202],\n",
              "          [0.6557, 0.9443, 0.5464, 0.7959, 0.9194, 0.9386]],\n",
              "\n",
              "         [[0.2063, 0.8837, 0.1892, 0.8777, 0.7307, 0.8780],\n",
              "          [0.1936, 0.4767, 0.0342, 0.8445, 0.2289, 0.9845],\n",
              "          [0.2448, 0.6513, 0.6890, 0.6405, 0.9350, 0.0735],\n",
              "          [0.5521, 0.3359, 0.7865, 0.0192, 0.4013, 0.8533],\n",
              "          [0.6124, 0.8746, 0.4929, 0.1250, 0.3675, 0.6932]],\n",
              "\n",
              "         [[0.7169, 0.4070, 0.2584, 0.6570, 0.1299, 0.3868],\n",
              "          [0.9336, 0.0314, 0.0702, 0.0835, 0.7599, 0.7893],\n",
              "          [0.0206, 0.2392, 0.6651, 0.8994, 0.5525, 0.5739],\n",
              "          [0.6196, 0.0658, 0.2004, 0.6260, 0.3884, 0.7152],\n",
              "          [0.1598, 0.5350, 0.5837, 0.5414, 0.0122, 0.8312]],\n",
              "\n",
              "         [[0.4999, 0.9954, 0.5138, 0.2207, 0.6739, 0.3542],\n",
              "          [0.5587, 0.5887, 0.1204, 0.2383, 0.9668, 0.7831],\n",
              "          [0.7456, 0.0442, 0.4065, 0.8939, 0.0641, 0.9126],\n",
              "          [0.7091, 0.5382, 0.0423, 0.1426, 0.5965, 0.9461],\n",
              "          [0.5574, 0.1777, 0.1607, 0.1285, 0.5276, 0.8786]]]])"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install einops"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bHUGQgJDurLs",
        "outputId": "aac036e3-0367-4531-943a-ffa8041cbee6"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting einops\n",
            "  Downloading einops-0.6.1-py3-none-any.whl (42 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/42.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.2/42.2 kB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: einops\n",
            "Successfully installed einops-0.6.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from einops import rearrange, reduce, repeat"
      ],
      "metadata": {
        "id": "lY_hSuTEth8L"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rand_tensor.size()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DtZy-gqnvNMr",
        "outputId": "11910772-56a1-4813-8153-7db91a9dc800"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([3, 4, 5, 6])"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rand_tensor = rearrange(rand_tensor, \"b c h w -> (b w) c h\")\n",
        "\n",
        "# b = 3, c = 4, h = 5, w = 6\n",
        "# to (3 * 6, 4, 5)"
      ],
      "metadata": {
        "id": "dTic0P1fu1ys"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rand_tensor.size()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LwpB-2HwvIRs",
        "outputId": "efe9135e-70b6-4ad3-8fd4-0225169c50de"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([18, 4, 5])"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### <b> einsum </b>"
      ],
      "metadata": {
        "id": "JE8HYgDKvzLu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "a = torch.rand((10, 10, 30))\n",
        "a.size()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RKuK0vagvKPd",
        "outputId": "7643b23f-1b23-440f-dc13-b86b5313aa86"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([10, 10, 30])"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "a = torch.rand(10, 20, 30)\n",
        "a.size()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f92IsGV5xz-9",
        "outputId": "7a10e41f-1ca7-4cfd-824a-293991daa7b1"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([10, 20, 30])"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "a = torch.randn(10, 20, 30)  # b = 10, i = 20, k = 30\n",
        "b = torch.randn(10, 50, 30)  # b = 10, i = 50, k = 30"
      ],
      "metadata": {
        "id": "pc3i2qQAx8Sy"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y = torch.einsum(\"b i k, b j k -> b i j\", a, b)"
      ],
      "metadata": {
        "id": "byKYLAJwyDXp"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y.size()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BjyYVOn8zw3S",
        "outputId": "e057d608-6e23-4d8d-ae79-6fae96d62807"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([10, 20, 50])"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "A = np.array([0, 1, 2])\n",
        "\n",
        "B = np.array([[ 0,  1,  2,  3],\n",
        "              [ 4,  5,  6,  7],\n",
        "              [ 8,  9, 10, 11]])"
      ],
      "metadata": {
        "id": "syj4A0QRzO90"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.einsum('i,ij->i', A, B)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ILoYfE8-zVhN",
        "outputId": "b5eebd8c-c3c2-4e63-f441-e679767854b8"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 0, 22, 76])"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# <b> Transformer's blocks implementation </b>"
      ],
      "metadata": {
        "id": "KoB3r7180w9U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <b> Scaled dot product self-attention </b>"
      ],
      "metadata": {
        "id": "t14xZSEi1QnJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the article, after embedding and positional embedding, embeddings are multiplied by weight matrices  <b> W_Q </b>,  <b> W_K </b>,  <b> W_V </b>."
      ],
      "metadata": {
        "id": "LsEIl3-U1Zvt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "the input <b> X </b> to the attention block is of shape: (batch, sequence_len, embedding_dim)"
      ],
      "metadata": {
        "id": "mLLcZraD2Sov"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The matrix multiplication happens in the <b> embedding_dim </b> dimension, regardless of batch_size and sequence_len"
      ],
      "metadata": {
        "id": "KJh9ObhI2j1Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# linear projection before attention block\n",
        "embed_dim = 512\n",
        "input = torch.randn(10, 12, 512)   # for example\n",
        "\n",
        "qkv_weights = nn.Linear(embed_dim, embed_dim * 3, bias = False)  # we concat q, k, v into one matrix, then multiply by qkv_weights\n",
        "\n",
        "qkv = qkv_weights(input)\n",
        "\n",
        "q, k, v = tuple(rearrange(qkv, \"b t (d k) -> k b t d\", k = 3))"
      ],
      "metadata": {
        "id": "V2XlrIKQzYXS"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "q.size(), k.size(), v.size()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5kogmTq039Ac",
        "outputId": "668f8bcf-1b9a-476e-9ce2-964ea008117a"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([10, 12, 512]),\n",
              " torch.Size([10, 12, 512]),\n",
              " torch.Size([10, 12, 512]))"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<b> Step 2 </b>"
      ],
      "metadata": {
        "id": "ouNjbWSJ6V2S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "calculate scaled dot product, apply mask, and finally compute softmax in d last dimension."
      ],
      "metadata": {
        "id": "32f77Wz36ZnR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# scaled dot product\n",
        "scale_factor = 512 / 8   # embed_dim / num_heads\n",
        "scaled_dot_product = torch.einsum(\"b i d, b j d -> b i j\",q, k) * scale_factor\n",
        "# resulting shape: (batch_size, embed_dim, embed_dim)\n",
        "\n",
        "# masking if needed (decoder)\n",
        "mask = None\n",
        "if mask is not None:\n",
        "  assert mask.shape == scaled_dot_product.shape[1:]\n",
        "  scaled_dot_product = scaled_dot_product.masked_fill(mask, -np.inf)\n",
        "\n",
        "attention = torch.softmax(scaled_dot_product, dim = -1)\n",
        "\n",
        "# multiply attention scores with V\n",
        "print(f\"attention size: {attention.size()}\")\n",
        "print(f\"value size: {v.size()}\")\n",
        "\n",
        "attention_final = torch.einsum(\"b i d, b d j -> b i j\", attention, v)"
      ],
      "metadata": {
        "id": "XUBdVuGu4AO2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e56068cf-f820-4d93-bd83-0479c7b0bdba"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "attention size: torch.Size([10, 12, 12])\n",
            "value size: torch.Size([10, 12, 512])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <b> Implementation of Scaled dot Prodcut Self-Attention </b>"
      ],
      "metadata": {
        "id": "PMWG8hwjgFq_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "import numpy\n",
        "from einops import rearrange"
      ],
      "metadata": {
        "id": "DMeN12Xkfe2Z"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SelfAttention(nn.Module):\n",
        "\n",
        "  def __init__(self, embed_dim):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        embed_dim: embedding dimension, with 512 as default\n",
        "        the last dimension size that is provided in forward(x), where x is a 3D tensor\n",
        "    \"\"\"\n",
        "    super().__init__()\n",
        "\n",
        "    # wieght matrices for query, key, and value\n",
        "    self.qkv_weights = nn.Linear(embed_dim, embed_dim * 3, bias = False)\n",
        "    self.scale_factor = embed_dim ** (-0.5)\n",
        "\n",
        "  # forward method\n",
        "  def forward(self, x, mask = None):\n",
        "    assert x.dim() == 3  # x must be a 3D tensor (batch_size, sequence_len, embed_dim)\n",
        "\n",
        "    # step 1\n",
        "    qkv = self.qkv_weights(x)\n",
        "\n",
        "    # step 2: decomposing to q, k, v\n",
        "    # rearranging to [3, batch_size, sequence_len, embed_dim]\n",
        "    q, k , v = tuple(rearrange(qkv, \"b t (d k) -> k b t d\", k = 3))\n",
        "\n",
        "    # scaled_dot_product\n",
        "    scaled_dot_product = torch.einsum(\"b i d, b j d -> b i j\", q, k) * self.scale_factor\n",
        "\n",
        "    # mask attention\n",
        "    if mask is not None:\n",
        "      assert mask.shape == x.shape[1:]\n",
        "      scaled_dot_product = scaled_dot_product.masked_fill(mask, -np.inf)\n",
        "\n",
        "    attention = torch.softmax(scaled_dot_product, dim = -1)\n",
        "    # you have to multiply V in the dimension you apply the softmax. Be careful of that.\n",
        "    attention_final = torch.einsum(\"b i d, b d k -> b i k\")\n",
        "\n",
        "    return attention_final"
      ],
      "metadata": {
        "id": "DNridq4ihU9D"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <b> Implementing Multi-Head_Self-Attention </b>"
      ],
      "metadata": {
        "id": "-BC8vz4Jqq6o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "in a single head case, we project the embedded and positioonal encoded input to weight matrix of size (embed_dim, embed_dim * 3)"
      ],
      "metadata": {
        "id": "g0CWPMRqq6Rm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "in multi-head case, we project the inuput matrix to weigh matrix of size (embed_dim, head_dim * n_heads * 3), which is the same, but in the `rearrange()` step it is easy to separate heads."
      ],
      "metadata": {
        "id": "QXCxh4fOrNTj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "head_dim = 512 // 8\n",
        "n_heads = 8\n",
        "x = torch.randn(10, 12, 512)\n",
        "\n",
        "qkv_weights = nn.Linear(embed_dim, head_dim * n_heads * 3, bias=False)\n",
        "qkv = qkv_weights(x)"
      ],
      "metadata": {
        "id": "6AM9ST-TnKgs"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# decompose qkv to q, k, v\n",
        "q, k, v = tuple(rearrange(qkv, \"b s (h n k) -> k b n s h\", k = 3, n = n_heads))"
      ],
      "metadata": {
        "id": "rkIt0sDPtTrG"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "qkv.size()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l44RQaqkuRGN",
        "outputId": "0cc37fa7-cd88-4cd4-90ad-5efc7fdad1e2"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([10, 12, 1536])"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "q.size(), k.size(), v.size()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_unn1LKeuSiX",
        "outputId": "bb95b7e2-74fd-459e-c05a-fcaca07e287a"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([10, 8, 12, 64]),\n",
              " torch.Size([10, 8, 12, 64]),\n",
              " torch.Size([10, 8, 12, 64]))"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "the next step is to calculate `scaled-dot-product`, apply the mask, and finally compute the `softmax` in `dim_head`"
      ],
      "metadata": {
        "id": "1vbqDUcIyLnd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# matrix multiplication of q and v in heads\n",
        "# q, k -> (batch_size, n_heads, seq_len, dim_head)\n",
        "scaled_dot_product = torch.einsum(\"b n s d, b n t d -> b n s t\", q, k) * scale_factor   # (batch_size, n_heads, seq_len, tokens) \"seq_len = tokens\"\n",
        "\n",
        "if mask is not None:\n",
        "  # check mask shape\n",
        "  assert mask.shape == scaled_dot_product.shape[2:]\n",
        "  scaled_dot_product = scaled_dot_product.masked_fill(mask, -np.inf)\n",
        "\n",
        "attention = torch.softmax(scaled_dot_product, dim = -1)"
      ],
      "metadata": {
        "id": "bG6HfP-wu8wM"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "now that the attention is computed, we must multiply the attention score of each head with the corresponding value of each head"
      ],
      "metadata": {
        "id": "aUssAlQh0M3U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# attention shape : (batch_size, n_heads, sentence_words, sentence_words)\n",
        "# value shape : (batch_size, n_heads, sentence_words, head_dim)\n",
        "\n",
        "out = torch.einsum(\"b n i j, b n j d -> b n i d\", attention, v)"
      ],
      "metadata": {
        "id": "dbyUz-150KI8"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "it's time to merge heads into one matrix and multiply it by W_O"
      ],
      "metadata": {
        "id": "xvDW-T_v2WUi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# concatenating heads\n",
        "out = rearrange(out, \"b n i d -> b i (n d)\")"
      ],
      "metadata": {
        "id": "Ubsr5GQq2TOi"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "MultiHead(Q, K, V) = Concat(head1, ..., head8) <br>\n",
        "head_i = Attention(QW_Q,i, KW_K,i, VW_V,i)"
      ],
      "metadata": {
        "id": "uFNr4IzA3-Ih"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# apply linear transformation W_O\n",
        "W_O = nn.Linear(embed_dim, embed_dim, bias = False)\n",
        "\n",
        "final_output = W_O(out)"
      ],
      "metadata": {
        "id": "NP-y8RFV3uyd"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <b> Implementation of Multi-Head-Attention </b>"
      ],
      "metadata": {
        "id": "7oUbH_DY5MJ2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from einops import rearrange\n",
        "import numpy as np\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "  def __init__(self, embed_dim, n_heads):\n",
        "    super().__init__()\n",
        "\n",
        "    self.n_heads = n_heads\n",
        "    self.dim_head = embed_dim // self.n_heads\n",
        "    self.embed_dim = self.dim_head * n_heads\n",
        "    self.scale_factor = self.dim_head ** (-0.5)\n",
        "\n",
        "    # weight matrices\n",
        "    self.qkv_weights = nn.Linear(self.embed_dim, n_heads * self.dim_head * 3, bias = False)\n",
        "    self.W_O = nn.Linear(self.embed_dim, self.embed_dim, bias = False)\n",
        "\n",
        "  # forward method\n",
        "  def forward(self, x, mask = None):\n",
        "    # check x has all 3 dimensions -> (batch_size, sentence_length, embedding_dim)\n",
        "    assert x.shape == 3\n",
        "\n",
        "    # step 1: compute query, key, value\n",
        "    qkv = self.qdv_weights(x)   # (batch_size, sentence_length, dim_head * n_heads * 3)\n",
        "\n",
        "    # step 2: decompose to q, k, v\n",
        "    # resulting shape befor tuple():\n",
        "    # [3, n_heads, batch_size, sentence_len, head_dim]\n",
        "    q, k, v = tuple(rearrange(qkv, \"b s (d n k) -> k b n s d\", k = 3, h = self.n_heads))\n",
        "\n",
        "    # step 3: compute scaled_dot_product\n",
        "    scaled_dot_product = torch.einsum(\"b n s d, h n t d -> b n s t\", q, v) * self.scale_factor\n",
        "\n",
        "    # mask if needed\n",
        "    if mask is not None:\n",
        "      assert mask.shape == scaled_dot_product[2:]\n",
        "      scaled_dot_product = scaled_dot_product.masked_fill(mask, -np.inf)\n",
        "\n",
        "    attention = torch.softmax(scaled_dot_product, dim = -1)\n",
        "\n",
        "    # step 4: calculate output\n",
        "    out = torch.einsum(\"b n s j, b n j d -> b n s d\", attention, v)\n",
        "\n",
        "    # step 5: merge heads\n",
        "    out = rearrange(out, \"b n i d -> b s (n d)\")\n",
        "\n",
        "    # step 6: apply W_O\n",
        "    output = self.W_O(out)\n",
        "\n",
        "    return output"
      ],
      "metadata": {
        "id": "ez4ngwIJ5KBo"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <b> Transformer Encoder Block </b>"
      ],
      "metadata": {
        "id": "N_u9fXbtOpjF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerBlock(nn.Module):\n",
        "\n",
        "  def __init__(self, dim, heads = 8, dim_head = None, dim_linear_block = 1024, dropout = 0.1):\n",
        "    \"\"\"\n",
        "    dim: tocken's vector lenght\n",
        "    heads: number of heads\n",
        "    dim_head: if None, dim / heads is used\n",
        "    dim_linear_block: linear projection dimension following MultiHeadAttention\n",
        "    dropout: probability of dropping out\n",
        "    \"\"\"\n",
        "    super().__init__()\n",
        "    self.mha = MultiHeadAttention(embed_dim = 512, n_heads = heads)\n",
        "    self.drop = nn.Dropout(dropout)\n",
        "    self.norm1 = nn.LayerNorm(dim)\n",
        "    self.norm2 = nn.LayerNorm(dim)\n",
        "\n",
        "    self.linear = nn.Sequential(\n",
        "        nn.Linear(dim, dim_linear_block),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(dim_linear_block, dim),\n",
        "        nn.Dropout(dropout)\n",
        "    )\n",
        "\n",
        "  # forward method\n",
        "  def forward(self, x, mask = None):\n",
        "    y = self.norm1(self.drop(self.mha(x, mask) + x))\n",
        "    return self.norm2(self.linear(y) + y)"
      ],
      "metadata": {
        "id": "ZjYcN66obBD4"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Qh_PoGd8f1A9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}