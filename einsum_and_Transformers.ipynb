{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM9Exds8C7P9cDeAxjYGFMB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mirpouya/Transformer_EDU/blob/main/einsum_and_Transformers.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Think about the situation that you want to merge d dimsnsion of a 4D matrix"
      ],
      "metadata": {
        "id": "LSPFzSl9biHm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "QccsvTiNbcSe"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rand_tensor = torch.rand((2, 5))\n",
        "rand_tensor"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m915A_Bob1jk",
        "outputId": "d1b6d6bf-eb76-416d-d9d3-6ef03ccf0ccf"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.7209, 0.9924, 0.4298, 0.3232, 0.9505],\n",
              "        [0.4455, 0.0844, 0.5883, 0.6810, 0.4742]])"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rand_tensor = torch.rand((3,4,5,6))\n",
        "rand_tensor"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XzLHL-fIb_5B",
        "outputId": "0b955860-0363-4888-f723-a9e20e8ece80"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[[4.7069e-03, 3.1583e-01, 8.0843e-01, 7.0262e-01, 6.3000e-01,\n",
              "           8.8252e-01],\n",
              "          [6.3940e-01, 7.5962e-01, 7.7184e-03, 3.4930e-01, 1.5440e-01,\n",
              "           9.5386e-01],\n",
              "          [2.4964e-01, 3.8791e-02, 6.9726e-01, 5.7344e-01, 7.9127e-01,\n",
              "           3.1802e-01],\n",
              "          [7.1218e-01, 5.6754e-01, 3.3723e-01, 7.7457e-01, 1.6603e-01,\n",
              "           6.7462e-01],\n",
              "          [2.3104e-01, 8.0137e-01, 6.9314e-03, 9.6764e-01, 6.3774e-01,\n",
              "           2.3832e-02]],\n",
              "\n",
              "         [[1.2354e-01, 1.6340e-01, 2.4800e-01, 9.6756e-01, 1.6925e-01,\n",
              "           7.2579e-01],\n",
              "          [8.7623e-02, 3.0110e-01, 6.9265e-01, 6.2613e-02, 4.4115e-01,\n",
              "           2.8838e-01],\n",
              "          [2.2816e-01, 3.3861e-01, 7.1474e-01, 5.1206e-01, 7.9921e-02,\n",
              "           5.9922e-01],\n",
              "          [6.0421e-01, 4.2378e-01, 8.7561e-01, 2.3363e-01, 5.8215e-01,\n",
              "           2.6234e-01],\n",
              "          [2.1714e-01, 1.3553e-01, 4.6720e-01, 7.1148e-01, 9.6144e-01,\n",
              "           8.2240e-01]],\n",
              "\n",
              "         [[5.6138e-01, 1.9346e-01, 9.4494e-01, 6.7343e-01, 8.1010e-01,\n",
              "           1.4869e-01],\n",
              "          [2.7042e-01, 2.8407e-01, 3.8349e-01, 3.6828e-03, 3.7838e-01,\n",
              "           6.4701e-01],\n",
              "          [9.3801e-01, 9.9197e-01, 6.1172e-01, 4.3408e-01, 5.1931e-01,\n",
              "           4.2122e-01],\n",
              "          [7.8179e-01, 6.9186e-01, 8.4650e-01, 9.2739e-02, 7.0109e-01,\n",
              "           7.2765e-01],\n",
              "          [5.3760e-01, 2.7290e-01, 6.3277e-01, 9.6436e-01, 9.4750e-01,\n",
              "           7.0755e-01]],\n",
              "\n",
              "         [[5.2797e-01, 3.4448e-02, 5.3734e-02, 7.3793e-01, 6.6026e-01,\n",
              "           5.4535e-01],\n",
              "          [4.8555e-01, 4.1002e-01, 7.7115e-01, 3.3561e-02, 7.9887e-01,\n",
              "           6.6861e-01],\n",
              "          [8.3599e-02, 4.1396e-01, 9.8739e-01, 7.2262e-01, 2.1583e-03,\n",
              "           7.6501e-01],\n",
              "          [2.5142e-01, 3.7783e-01, 4.0994e-01, 7.6591e-01, 7.3082e-01,\n",
              "           3.4931e-01],\n",
              "          [9.1310e-01, 2.4601e-02, 3.5153e-01, 4.5792e-01, 4.6139e-01,\n",
              "           3.7575e-01]]],\n",
              "\n",
              "\n",
              "        [[[9.1583e-01, 7.2739e-01, 9.4995e-02, 6.6941e-01, 5.4664e-02,\n",
              "           7.1208e-01],\n",
              "          [6.7129e-01, 7.2780e-02, 5.3850e-01, 9.2239e-01, 5.0485e-01,\n",
              "           2.1011e-01],\n",
              "          [7.0548e-01, 6.4039e-01, 2.4670e-01, 8.9126e-01, 8.3409e-01,\n",
              "           7.7922e-01],\n",
              "          [4.5310e-01, 9.6278e-01, 2.1139e-01, 4.8646e-01, 9.5684e-01,\n",
              "           3.4361e-01],\n",
              "          [1.5724e-01, 1.2737e-02, 7.9295e-01, 3.0036e-01, 7.9918e-01,\n",
              "           6.3828e-01]],\n",
              "\n",
              "         [[2.5529e-01, 2.4735e-01, 1.7370e-01, 4.6390e-01, 4.8581e-01,\n",
              "           4.4335e-01],\n",
              "          [1.0758e-01, 2.9205e-01, 3.4810e-01, 6.4762e-02, 2.7919e-01,\n",
              "           9.0306e-01],\n",
              "          [2.6350e-01, 6.1759e-01, 2.7437e-01, 8.7297e-04, 2.2713e-01,\n",
              "           4.5913e-01],\n",
              "          [7.8490e-01, 3.8124e-01, 4.7909e-01, 6.2170e-01, 3.7754e-02,\n",
              "           2.5415e-01],\n",
              "          [8.7064e-01, 9.1425e-01, 5.3350e-01, 4.2691e-01, 5.0239e-01,\n",
              "           9.5732e-01]],\n",
              "\n",
              "         [[6.2074e-01, 6.3514e-01, 4.4390e-01, 9.0946e-01, 7.2834e-01,\n",
              "           7.4732e-01],\n",
              "          [2.8653e-01, 7.8134e-01, 8.1307e-01, 4.4184e-01, 9.1262e-01,\n",
              "           9.3264e-01],\n",
              "          [2.5399e-01, 5.4001e-01, 3.5262e-01, 5.3729e-01, 4.5237e-01,\n",
              "           4.5589e-01],\n",
              "          [1.1706e-01, 9.5639e-01, 8.0557e-01, 8.0014e-01, 8.8123e-01,\n",
              "           8.9163e-01],\n",
              "          [5.9079e-01, 4.5966e-01, 6.9648e-01, 8.8221e-01, 5.7132e-01,\n",
              "           4.7879e-01]],\n",
              "\n",
              "         [[5.8301e-01, 5.3218e-01, 7.2627e-02, 8.8049e-01, 1.9081e-02,\n",
              "           7.1747e-01],\n",
              "          [3.2311e-01, 9.6070e-02, 6.8654e-01, 5.5013e-01, 5.0436e-01,\n",
              "           8.2872e-01],\n",
              "          [5.6207e-01, 2.9210e-01, 8.3433e-01, 5.8607e-01, 7.0580e-01,\n",
              "           6.7853e-01],\n",
              "          [8.0285e-01, 8.7646e-01, 1.7988e-01, 4.4514e-01, 6.8676e-01,\n",
              "           7.5597e-01],\n",
              "          [4.2240e-01, 8.7728e-01, 2.5396e-01, 8.7148e-01, 4.7266e-01,\n",
              "           8.2820e-01]]],\n",
              "\n",
              "\n",
              "        [[[2.4398e-01, 7.6739e-01, 5.2148e-01, 1.9442e-01, 5.9610e-01,\n",
              "           7.5622e-01],\n",
              "          [5.2170e-02, 4.7785e-01, 8.5815e-01, 3.0138e-01, 5.9702e-01,\n",
              "           1.5117e-01],\n",
              "          [4.1132e-01, 9.8179e-01, 4.3646e-01, 7.9227e-01, 6.8829e-01,\n",
              "           3.5025e-01],\n",
              "          [6.8903e-01, 8.3634e-01, 8.9907e-01, 3.9910e-01, 5.9580e-01,\n",
              "           7.5212e-01],\n",
              "          [6.9180e-01, 1.5469e-01, 9.5360e-01, 7.1285e-01, 3.3359e-01,\n",
              "           1.4859e-01]],\n",
              "\n",
              "         [[2.3856e-01, 9.5884e-01, 7.5944e-01, 1.9219e-02, 2.4681e-01,\n",
              "           2.2167e-01],\n",
              "          [3.4851e-01, 4.0347e-01, 1.6968e-01, 3.4819e-01, 8.7089e-01,\n",
              "           9.4942e-01],\n",
              "          [1.4725e-01, 3.4939e-01, 5.0662e-01, 6.8654e-01, 4.4133e-01,\n",
              "           7.5979e-01],\n",
              "          [5.6105e-01, 8.2172e-01, 2.0126e-01, 9.1964e-01, 8.2067e-01,\n",
              "           8.5356e-01],\n",
              "          [1.6179e-01, 7.5368e-01, 4.6493e-01, 6.6773e-01, 9.2153e-01,\n",
              "           5.6842e-01]],\n",
              "\n",
              "         [[6.4345e-01, 7.9991e-01, 8.5864e-02, 5.8673e-01, 1.5071e-01,\n",
              "           3.0760e-01],\n",
              "          [8.2601e-02, 3.3581e-01, 3.9743e-01, 8.4172e-01, 2.9772e-01,\n",
              "           6.1440e-01],\n",
              "          [6.3437e-01, 1.4960e-01, 3.9035e-01, 8.7370e-02, 4.6544e-01,\n",
              "           1.6777e-01],\n",
              "          [1.7086e-01, 1.3169e-01, 6.6461e-01, 1.2153e-01, 4.0105e-01,\n",
              "           1.6716e-01],\n",
              "          [3.4212e-01, 9.3513e-01, 7.9376e-01, 1.4168e-01, 3.9834e-01,\n",
              "           7.1407e-01]],\n",
              "\n",
              "         [[2.0153e-01, 5.4954e-01, 7.4509e-01, 6.1615e-01, 9.3899e-01,\n",
              "           5.4986e-01],\n",
              "          [3.5480e-01, 5.0814e-01, 6.1448e-01, 8.2569e-01, 4.3554e-01,\n",
              "           5.7769e-01],\n",
              "          [3.7890e-01, 5.3878e-01, 2.0069e-01, 1.6979e-01, 6.9800e-01,\n",
              "           9.1919e-01],\n",
              "          [4.5848e-01, 4.8716e-01, 9.0938e-01, 2.7973e-01, 3.7802e-01,\n",
              "           1.0269e-01],\n",
              "          [7.0468e-01, 7.4862e-01, 4.3816e-01, 1.8649e-01, 7.1358e-02,\n",
              "           9.0566e-01]]]])"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install einops"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bHUGQgJDurLs",
        "outputId": "f25113e6-469d-4e8c-957d-7e95ba0e5edc"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: einops in /usr/local/lib/python3.10/dist-packages (0.6.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from einops import rearrange, reduce, repeat"
      ],
      "metadata": {
        "id": "lY_hSuTEth8L"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rand_tensor.size()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DtZy-gqnvNMr",
        "outputId": "eff0d39b-0b44-45be-98ef-1ec91d06f609"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([3, 4, 5, 6])"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rand_tensor = rearrange(rand_tensor, \"b c h w -> (b w) c h\")\n",
        "\n",
        "# b = 3, c = 4, h = 5, w = 6\n",
        "# to (3 * 6, 4, 5)"
      ],
      "metadata": {
        "id": "dTic0P1fu1ys"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rand_tensor.size()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LwpB-2HwvIRs",
        "outputId": "a8a03e33-9de7-478b-9377-f15c242c4f64"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([18, 4, 5])"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### <b> einsum </b>"
      ],
      "metadata": {
        "id": "JE8HYgDKvzLu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "a = torch.rand((10, 10, 30))\n",
        "a.size()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RKuK0vagvKPd",
        "outputId": "54bdfcfb-6304-4afc-9e7a-0a334a88aa71"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([10, 10, 30])"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "a = torch.rand(10, 20, 30)\n",
        "a.size()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f92IsGV5xz-9",
        "outputId": "87f94820-ce25-49a4-e9cc-31c24f39884a"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([10, 20, 30])"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "a = torch.randn(10, 20, 30)  # b = 10, i = 20, k = 30\n",
        "b = torch.randn(10, 50, 30)  # b = 10, i = 50, k = 30"
      ],
      "metadata": {
        "id": "pc3i2qQAx8Sy"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y = torch.einsum(\"b i k, b j k -> b i j\", a, b)"
      ],
      "metadata": {
        "id": "byKYLAJwyDXp"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y.size()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BjyYVOn8zw3S",
        "outputId": "222df087-93f7-4fad-f8dc-27cb4ec456ab"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([10, 20, 50])"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "A = np.array([0, 1, 2])\n",
        "\n",
        "B = np.array([[ 0,  1,  2,  3],\n",
        "              [ 4,  5,  6,  7],\n",
        "              [ 8,  9, 10, 11]])"
      ],
      "metadata": {
        "id": "syj4A0QRzO90"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.einsum('i,ij->i', A, B)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ILoYfE8-zVhN",
        "outputId": "a53648b2-d878-4115-a83a-710ee4990a39"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 0, 22, 76])"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# <b> Transformer's blocks implementation </b>"
      ],
      "metadata": {
        "id": "KoB3r7180w9U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <b> Scaled dot product self-attention </b>"
      ],
      "metadata": {
        "id": "t14xZSEi1QnJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the article, after embedding and positional embedding, embeddings are multiplied by weight matrices  <b> W_Q </b>,  <b> W_K </b>,  <b> W_V </b>."
      ],
      "metadata": {
        "id": "LsEIl3-U1Zvt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "the input <b> X </b> to the attention block is of shape: (batch, sequence_len, embedding_dim)"
      ],
      "metadata": {
        "id": "mLLcZraD2Sov"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The matrix multiplication happens in the <b> embedding_dim </b> dimension, regardless of batch_size and sequence_len"
      ],
      "metadata": {
        "id": "KJh9ObhI2j1Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# linear projection before attention block\n",
        "embed_dim = 512\n",
        "input = torch.randn(10, 12, 512)   # for example\n",
        "\n",
        "qkv_weights = nn.Linear(embed_dim, embed_dim * 3, bias = False)  # we concat q, k, v into one matrix, then multiply by qkv_weights\n",
        "\n",
        "qkv = qkv_weights(input)\n",
        "\n",
        "q, k, v = tuple(rearrange(qkv, \"b t (d k) -> k b t d\", k = 3))"
      ],
      "metadata": {
        "id": "V2XlrIKQzYXS"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "q.size(), k.size(), v.size()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5kogmTq039Ac",
        "outputId": "46312c69-ddc3-4bc1-8f0b-0f3b452593c0"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([10, 12, 512]),\n",
              " torch.Size([10, 12, 512]),\n",
              " torch.Size([10, 12, 512]))"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<b> Step 2 </b>"
      ],
      "metadata": {
        "id": "ouNjbWSJ6V2S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "calculate scaled dot product, apply mask, and finally compute softmax in d last dimension."
      ],
      "metadata": {
        "id": "32f77Wz36ZnR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# scaled dot product\n",
        "scale_factor = 512 / 8   # embed_dim / num_heads\n",
        "scaled_dot_product = torch.einsum(\"b i d, b j d -> b i j\",q, k) * scale_factor\n",
        "# resulting shape: (batch_size, embed_dim, embed_dim)\n",
        "\n",
        "# masking if needed (decoder)\n",
        "mask = None\n",
        "if mask is not None:\n",
        "  assert mask.shape == scaled_dot_product.shape[1:]\n",
        "  scaled_dot_product = scaled_dot_product.masked_fill(mask, -np.inf)\n",
        "\n",
        "attention = torch.softmax(scaled_dot_product, dim = -1)\n",
        "\n",
        "# multiply attention scores with V\n",
        "print(f\"attention size: {attention.size()}\")\n",
        "print(f\"value size: {v.size()}\")\n",
        "\n",
        "attention_final = torch.einsum(\"b i d, b d j -> b i j\", attention, v)"
      ],
      "metadata": {
        "id": "XUBdVuGu4AO2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "39e47ea9-c256-49ec-980d-5c3d65cebb05"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "attention size: torch.Size([10, 12, 12])\n",
            "value size: torch.Size([10, 12, 512])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <b> Implementation of Scaled dot Prodcut Self-Attention </b>"
      ],
      "metadata": {
        "id": "PMWG8hwjgFq_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "import numpy\n",
        "from einops import rearrange"
      ],
      "metadata": {
        "id": "DMeN12Xkfe2Z"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SelfAttention(nn.Module):\n",
        "\n",
        "  def __init__(self, embed_dim):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        embed_dim: embedding dimension, with 512 as default\n",
        "        the last dimension size that is provided in forward(x), where x is a 3D tensor\n",
        "    \"\"\"\n",
        "    super().__init__()\n",
        "\n",
        "    # wieght matrices for query, key, and value\n",
        "    self.qkv_weights = nn.Linear(embed_dim, embed_dim * 3, bias = False)\n",
        "    self.scale_factor = embed_dim ** (-0.5)\n",
        "\n",
        "  # forward method\n",
        "  def forward(self, x, mask = None):\n",
        "    assert x.dim() == 3  # x must be a 3D tensor (batch_size, sequence_len, embed_dim)\n",
        "\n",
        "    # step 1\n",
        "    qkv = self.qkv_weights(x)\n",
        "\n",
        "    # step 2: decomposing to q, k, v\n",
        "    # rearranging to [3, batch_size, sequence_len, embed_dim]\n",
        "    q, k , v = tuple(rearrange(qkv, \"b t (d k) -> k b t d\", k = 3))\n",
        "\n",
        "    # scaled_dot_product\n",
        "    scaled_dot_product = torch.einsum(\"b i d, b j d -> b i j\", q, k) * self.scale_factor\n",
        "\n",
        "    # mask attention\n",
        "    if mask is not None:\n",
        "      assert mask.shape == x.shape[1:]\n",
        "      scaled_dot_product = scaled_dot_product.masked_fill(mask, -np.inf)\n",
        "\n",
        "    attention = torch.softmax(scaled_dot_product, dim = -1)\n",
        "    # you have to multiply V in the dimension you apply the softmax. Be careful of that.\n",
        "    attention_final = torch.einsum(\"b i d, b d k -> b i k\")\n",
        "\n",
        "    return attention_final"
      ],
      "metadata": {
        "id": "DNridq4ihU9D"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <b> Implementing Multi-Head_Self-Attention </b>"
      ],
      "metadata": {
        "id": "-BC8vz4Jqq6o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "in a single head case, we project the embedded and positioonal encoded input to weight matrix of size (embed_dim, embed_dim * 3)"
      ],
      "metadata": {
        "id": "g0CWPMRqq6Rm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "in multi-head case, we project the inuput matrix to weigh matrix of size (embed_dim, head_dim * n_heads * 3), which is the same, but in the `rearrange()` step it is easy to separate heads."
      ],
      "metadata": {
        "id": "QXCxh4fOrNTj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "head_dim = 512 // 8\n",
        "n_heads = 8\n",
        "x = torch.randn(10, 12, 512)\n",
        "\n",
        "qkv_weights = nn.Linear(embed_dim, head_dim * n_heads * 3, bias=False)\n",
        "qkv = qkv_weights(x)"
      ],
      "metadata": {
        "id": "6AM9ST-TnKgs"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# decompose qkv to q, k, v\n",
        "q, k, v = tuple(rearrange(qkv, \"b s (h n k) -> k b n s h\", k = 3, n = n_heads))"
      ],
      "metadata": {
        "id": "rkIt0sDPtTrG"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "qkv.size()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l44RQaqkuRGN",
        "outputId": "8fd962d9-ec5d-4d29-9c1f-287940ad787e"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([10, 12, 1536])"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "q.size(), k.size(), v.size()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_unn1LKeuSiX",
        "outputId": "786c4db9-8f9e-48e0-aa5a-c34560db529c"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([10, 8, 12, 64]),\n",
              " torch.Size([10, 8, 12, 64]),\n",
              " torch.Size([10, 8, 12, 64]))"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "the next step is to calculate `scaled-dot-product`, apply the mask, and finally compute the `softmax` in `dim_head`"
      ],
      "metadata": {
        "id": "1vbqDUcIyLnd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# matrix multiplication of q and v in heads\n",
        "# q, k -> (batch_size, n_heads, seq_len, dim_head)\n",
        "scaled_dot_product = torch.einsum(\"b n s d, b n t d -> b n s t\", q, k) * scale_factor   # (batch_size, n_heads, seq_len, tokens) \"seq_len = tokens\"\n",
        "\n",
        "if mask is not None:\n",
        "  # check mask shape\n",
        "  assert mask.shape == scaled_dot_product.shape[2:]\n",
        "  scaled_dot_product = scaled_dot_product.masked_fill(mask, -np.inf)\n",
        "\n",
        "attention = torch.softmax(scaled_dot_product, dim = -1)"
      ],
      "metadata": {
        "id": "bG6HfP-wu8wM"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "now that the attention is computed, we must multiply the attention score of each head with the corresponding value of each head"
      ],
      "metadata": {
        "id": "aUssAlQh0M3U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# attention shape : (batch_size, n_heads, sentence_words, sentence_words)\n",
        "# value shape : (batch_size, n_heads, sentence_words, head_dim)\n",
        "\n",
        "out = torch.einsum(\"b n i j, b n j d -> b n i d\", attention, v)"
      ],
      "metadata": {
        "id": "dbyUz-150KI8"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "it's time to merge heads into one matrix and multiply it by W_O"
      ],
      "metadata": {
        "id": "xvDW-T_v2WUi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# concatenating heads\n",
        "out = rearrange(out, \"b n i d -> b i (n d)\")"
      ],
      "metadata": {
        "id": "Ubsr5GQq2TOi"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "MultiHead(Q, K, V) = Concat(head1, ..., head8) <br>\n",
        "head_i = Attention(QW_Q,i, KW_K,i, VW_V,i)"
      ],
      "metadata": {
        "id": "uFNr4IzA3-Ih"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# apply linear transformation W_O\n",
        "W_O = nn.Linear(embed_dim, embed_dim, bias = False)\n",
        "\n",
        "final_output = W_O(out)"
      ],
      "metadata": {
        "id": "NP-y8RFV3uyd"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <b> Implementation of Multi-Head-Attention </b>"
      ],
      "metadata": {
        "id": "7oUbH_DY5MJ2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from einops import rearrange\n",
        "import numpy as np\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "  def __init__(self, embed_dim, n_heads):\n",
        "    super().__init__()\n",
        "\n",
        "    self.n_heads = n_heads\n",
        "    self.dim_head = embed_dim // self.n_heads\n",
        "    self.embed_dim = self.dim_head * n_heads\n",
        "    self.scale_factor = self.dim_head ** (-0.5)\n",
        "\n",
        "    # weight matrices\n",
        "    self.qkv_weights = nn.Linear(self.embed_dim, n_heads * self.dim_head * 3, bias = False)\n",
        "    self.W_O = nn.Linear(self.embed_dim, self.embed_dim, bias = False)\n",
        "\n",
        "  # forward method\n",
        "  def forward(self, x, mask = None):\n",
        "    # check x has all 3 dimensions -> (batch_size, sentence_length, embedding_dim)\n",
        "    assert x.shape == 3\n",
        "\n",
        "    # step 1: compute query, key, value\n",
        "    qkv = self.qdv_weights(x)   # (batch_size, sentence_length, dim_head * n_heads * 3)\n",
        "\n",
        "    # step 2: decompose to q, k, v\n",
        "    # resulting shape befor tuple():\n",
        "    # [3, n_heads, batch_size, sentence_len, head_dim]\n",
        "    q, k, v = tuple(rearrange(qkv, \"b s (d n k) -> k b n s d\", k = 3, h = self.n_heads))\n",
        "\n",
        "    # step 3: compute scaled_dot_product\n",
        "    scaled_dot_product = torch.einsum(\"b n s d, h n t d -> b n s t\", q, v) * self.scale_factor\n",
        "\n",
        "    # mask if needed\n",
        "    if mask is not None:\n",
        "      assert mask.shape == scaled_dot_product[2:]\n",
        "      scaled_dot_product = scaled_dot_product.masked_fill(mask, -np.inf)\n",
        "\n",
        "    attention = torch.softmax(scaled_dot_product, dim = -1)\n",
        "\n",
        "    # step 4: calculate output\n",
        "    out = torch.einsum(\"b n s j, b n j d -> b n s d\", attention, v)\n",
        "\n",
        "    # step 5: merge heads\n",
        "    out = rearrange(out, \"b n i d -> b s (n d)\")\n",
        "\n",
        "    # step 6: apply W_O\n",
        "    output = self.W_O(out)\n",
        "\n",
        "    return output"
      ],
      "metadata": {
        "id": "ez4ngwIJ5KBo"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-wRGJ3uOL0Bq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}